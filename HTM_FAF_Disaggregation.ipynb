{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heavy Truck Model\n",
    "## FAF Annual Tonnage to Truck Number by TAZ Disaggregation\n",
    "\n",
    "Author: Maddie Hasani, Fehr & Peers <br/>\n",
    "Reviewer: Fatemeh Ranaiefar, Fehr & Peers<br/>\n",
    "Last update: 12/19/2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REQUIRED LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openmatrix as omx\n",
    "from scipy import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Display all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(path):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - Path: a full path to the input file. For example: 'C:\\GitLab\\HVM scripts\\input_file.xlsx'\n",
    "\n",
    "    input_file:\n",
    "    - HTM input excel file\n",
    "\n",
    "    Returns:\n",
    "    - df_dict (dict): A dictionary containing DataFrames for each sheet in the input Excel file.\n",
    "    - traffic_skims (DataFrame): Processed DataFrame containing traffic skims.\n",
    "\n",
    "    Description:\n",
    "    This function loads and preprocesses input files and returns DataFrames and processed traffic skims.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a dictionary to hold DataFrames\n",
    "    df_dict = {}\n",
    "\n",
    "    # 1. Load in input files\n",
    "    input_path = path\n",
    "    inputs_sandag_HTM = pd.ExcelFile(input_path)\n",
    "    sheet_names = [sheet_name for sheet_name in inputs_sandag_HTM.sheet_names if sheet_name.lower() not in ['userguide', 'reference']]\n",
    "\n",
    "    # 1.1 Load all sheets into separate DataFrames with lowercase names\n",
    "    for sheet_name in sheet_names:\n",
    "        df_name = sheet_name.lower()  # Convert sheet name to lowercase\n",
    "        df_dict[df_name] = inputs_sandag_HTM.parse(sheet_name)  # Save DataFrame to the dictionary\n",
    "\n",
    "    # 2. Load in FAF data\n",
    "    faf = df_dict['faf']  # Use the dictionary to get the DataFrame\n",
    "    faf_name = faf.loc[0, 'Name']\n",
    "    faf_path = faf.loc[0, 'Path']\n",
    "    full_faf_path = f\"{faf_path}\\\\{faf_name}.csv\"\n",
    "    df = pd.read_csv(full_faf_path)\n",
    "\n",
    "    # 3. Load in MGRA data\n",
    "    mgra_loc = df_dict['mgra']  # Use the dictionary to get the DataFrame\n",
    "    mgra_name = mgra_loc.loc[0, 'Name']\n",
    "    mgra_path = mgra_loc.loc[0, 'Path']\n",
    "    full_mgra_path = f\"{mgra_path}\\\\{mgra_name}.csv\"\n",
    "    mgra = pd.read_csv(full_mgra_path)\n",
    "\n",
    "    # 4. Load in Skim file\n",
    "    skim = df_dict['skim']  # Use the dictionary to get the DataFrame\n",
    "    skim_name = skim.loc[0, 'Name']\n",
    "    skim_path = skim.loc[0, 'Path']\n",
    "    full_skim_path = f\"{skim_path}\\\\{skim_name}.omx\"\n",
    "\n",
    "    traffic_skims_PM = []\n",
    "    with omx.open_file(full_skim_path) as omx_file:\n",
    "        for name in [\"PM_TRK_H_DIST\"]:\n",
    "            matrix = omx_file[name]\n",
    "            df_skim = pd.DataFrame(matrix[:])\n",
    "            stacked_df = df_skim.stack().reset_index()  # Reset the index to make it separate columns\n",
    "            stacked_df.columns = [\"origin\", \"destination\", name]  # Rename the columns\n",
    "            traffic_skims_PM.append(stacked_df)\n",
    "    traffic_skims = pd.concat(traffic_skims_PM, axis=1)\n",
    "\n",
    "    # 5. Filter out OD pairs where at least one end is a gateway\n",
    "    traffic_skims = traffic_skims.loc[(traffic_skims['origin'] >= 13) & (traffic_skims['destination'] >= 13)]\n",
    "\n",
    "    return df, mgra, traffic_skims, df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_faf(df, faz_gateway, faz_county, sd_flows, othermode_truck, commodity_group):\n",
    "    \"\"\"\n",
    "    Process a DataFrame containing transportation data.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame containing transportation data.\n",
    "    - faz_gateway (DataFrame): DataFrame containing FAZ and Gateways/Airports/Ports TAZ.\n",
    "    - faz_county (DataFrame): DataFrame containing FAZ and County information.\n",
    "    - sd_flows (DataFrame): DataFrame containing Origin and Destination pair that will generate truck trips to/from SANDAG or Pass through SANDAG region.\n",
    "    - othermode_truck (DataFrame): DataFrame with mode information and percentages.\n",
    "    - commodity_group (DataFrame): DataFrame mapping commodities to CG values.\n",
    "\n",
    "    Returns:\n",
    "    - processed_df (DataFrame): Processed DataFrame with updated columns.\n",
    "\n",
    "    Description:\n",
    "    This function takes FAF data and performs the following steps:\n",
    "    1. Determine what are the OD pairs that may pass through SANDAG region or start/end in SANDAG\n",
    "    2. Includes only specified modes and calculates tonnage.\n",
    "    3. Deletes unnecessary columns.\n",
    "    4. Assigns SANDAG commodity groups based on SCTG commodity group.\n",
    "    5. Aggregates tonnage data by OD FAZ and Commodity Group.\n",
    "    6. Adds columns identifying if OD pairs have at least one end within Orange County or Mexico.\n",
    "    The processed DataFrame is returned.\n",
    "    \"\"\"\n",
    "    sd_flows = df_dict[sd_flows]\n",
    "    faz_gateway = df_dict[faz_gateway]\n",
    "    faz_county = df_dict[faz_county]\n",
    "    othermode_truck = df_dict[othermode_truck]\n",
    "    commodity_group = df_dict[commodity_group]\n",
    "\n",
    "\n",
    "    # 1. Determine what are the OD pairs that may pass through SANDAG region or start/end in SANDAG\n",
    "    # Mapping columns 'dms_orig' and 'dms_dest' based on the lookup table to determine the area code (check out the reference tab in the input spreadsheet for more info)\n",
    "    df = df.merge(faz_gateway[['FAZ', 'AreaCode']], how='left', left_on='dms_orig', right_on='FAZ').drop('FAZ', axis=1).rename(columns={'AreaCode': 'code_orig'})\n",
    "    df = df.merge(faz_gateway[['FAZ', 'AreaCode']], how='left', left_on='dms_dest', right_on='FAZ').drop('FAZ', axis=1).rename(columns={'AreaCode': 'code_dest'})\n",
    "    # if FAZ is within SANDAG, the areacode should be 8\n",
    "    faz_sandag = faz_county[faz_county[\"County\"] == \"San Diego\"][\"FAZ\"]\n",
    "    df.loc[df['dms_orig'].isin(faz_sandag), 'code_orig'] = 8\n",
    "    df.loc[df['dms_dest'].isin(faz_sandag), 'code_dest'] = 8\n",
    "    # merge the distribution of truck trips between each OD pair\n",
    "    df = df.merge(sd_flows[['OriginCode', 'DestinationCode', 'Dist']], how='left', left_on=['code_orig', 'code_dest'], right_on=['OriginCode', 'DestinationCode']).drop(['OriginCode', 'DestinationCode'], axis=1)\n",
    "    # calculate how much of each OD pairs will travel through SANDAG - multiply by the ton\n",
    "    df['distons_2025'] = df['distons_2025'] * df['Dist']\n",
    "    # remove all OD pairs that do not have any tons traveled through or within SANDAG\n",
    "    df = df[df['distons_2025'] > 0]\n",
    "    df.drop(['Dist'], axis=1, inplace=True)\n",
    "\n",
    "    # 2. Include some modes\n",
    "    mode_to_include = othermode_truck.set_index('Mode_Num')['Percentage'].to_dict()\n",
    "    df = df[df['Mode'].isin(mode_to_include.keys())]\n",
    "    df['truck_perc'] = df['Mode'].map(mode_to_include)\n",
    "    df['ton'] = df['distons_2025'] * df['truck_perc']\n",
    "    df.drop(['truck_perc', 'distons_2025'], axis=1, inplace=True)\n",
    "\n",
    "    # 3. Delete unnecessary columns\n",
    "    delete_col = ['Direction', 'Trade', 'disvalue_2017', 'distons_2017', 'disvalue_2025', 'distons_2030', 'disvalue_2030', 'distons_2035', \n",
    "                    'disvalue_2035', 'distons_2040', 'disvalue_2040', 'distons_2045', 'disvalue_2045', 'distons_2050', 'disvalue_2050'] \n",
    "    df.drop(delete_col, axis=1, inplace=True)\n",
    "\n",
    "    # 4. Assign SANDAG commodity groups based on SCTG commodity group\n",
    "    commodity_to_cg = commodity_group.set_index('SCTG')['CG'].to_dict()\n",
    "    df['CG'] = df['Commodity'].map(commodity_to_cg)\n",
    "    df.drop('Commodity', axis=1, inplace=True)\n",
    "\n",
    "    # 5. Aggregate the Tonnage Data by Origin/Destination and Commodity Group\n",
    "    df['fr_orig'] = df['fr_orig'].fillna(0)\n",
    "    df['fr_dest'] = df['fr_dest'].fillna(0)\n",
    "    df = df.groupby(['dms_orig', 'dms_dest', 'fr_orig', 'fr_dest', 'code_orig', 'code_dest', 'CG'], as_index=False).agg({'ton': 'sum'})\n",
    "\n",
    "    # 6. Create a new column that identifies if at least one end of the OD is within Orange County and mexico\n",
    "    df['one_end_orange'] = ((df['code_orig'] == 6) | (df['code_dest'] == 6)).astype(int)\n",
    "    df['one_end_mx'] = ((df['fr_orig'].isin([802])) | (df['fr_dest'].isin([802]))).astype(int)                            \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_taz_data(mgra, faz_county, emp_converter, taz_faz):\n",
    "    \"\"\"\n",
    "    Prepare TAZ data by calculating NAICS employee category percentages within each TAZ.\n",
    "\n",
    "    Parameters:\n",
    "    - mgra (DataFrame): DataFrame containing MGRA data.\n",
    "    - faz_county (DataFrame): DataFrame containing FAZ and County information.\n",
    "    - emp_converter (DataFrame): DataFrame mapping SANDAG emp category to NAICS emp category.\n",
    "    - taz_faz (DataFrame): DataFrame mapping TAZ values to FAZ values.\n",
    "\n",
    "    Returns:\n",
    "    - taz_long (DataFrame): Processed DataFrame with calculated employee category percentages.\n",
    "\n",
    "    Description:\n",
    "    This function calculates the percentage of each employee category (from the NAICS Dataset)\n",
    "    within each TAZ. It performs the following steps:\n",
    "    1. Calculate the number of SADNAG employees by TAZ.\n",
    "    2. Add FAZ to the emp by TAZ table\n",
    "    3. Reformat the TAZ table to long format using emp_ and pop columns.\n",
    "    4. Reformat the emp_converter table to long format using FAZ columns\n",
    "    5. Convert SANDAG emp category to NAICS emp category.\n",
    "    6. Calculate the number of NAICS employees by TAZ.\n",
    "    7. Aggregate total number of each NAICS employee category by TAZ.\n",
    "    6. Create a mapping of TAZ values to FAZ values.\n",
    "    8. Calculate how many percentage of each emp category is within a TAZ\n",
    "    The processed DataFrame is returned.\n",
    "    \"\"\"\n",
    "\n",
    "    emp_converter = df_dict[emp_converter]\n",
    "    taz_faz = df_dict[taz_faz]\n",
    "    faz_county = df_dict[faz_county]\n",
    "    \n",
    "    # 1. Calculate the number of employee by TAZ\n",
    "    cols_to_sum = ['pop'] + [col for col in mgra.columns if col.startswith('emp_')]\n",
    "    taz = mgra.groupby(['taz'], as_index=False)[cols_to_sum].sum()\n",
    "\n",
    "    # 2. Add FAZ to the emp by TAZ table\n",
    "    taz_to_faz = taz_faz.set_index('TAZ')['FAZ'].to_dict()\n",
    "    # Use the map function to directly assign TAZ values\n",
    "    taz['FAZ'] = taz['taz'].map(taz_to_faz)\n",
    "\n",
    "    # 3. Reformat the taz table to long format using emp_ columns and pop\n",
    "    taz_long = taz.melt(id_vars=['taz', 'FAZ'], value_vars=cols_to_sum, value_name='sandag_emp_num', var_name='sandag_emp')\n",
    "\n",
    "    # 4. Reformat the emp_converter table to long format using FAZ columns\n",
    "    faz_sandag = faz_county[faz_county[\"County\"] == \"San Diego\"][\"FAZ\"]\n",
    "    emp_converter_long = emp_converter.melt(id_vars=['NAICS_Emp', 'SANDAG_Emp'], value_vars=faz_sandag, value_name='Emp_PCT', var_name='FAZ')\n",
    "\n",
    "    # 5. Convert SANDAG emp category to NAICS emp category\n",
    "    taz_long = emp_converter_long.merge(taz_long, how='inner', right_on=['sandag_emp', 'FAZ'], left_on=['SANDAG_Emp', 'FAZ']).drop(['sandag_emp'], axis=1)\n",
    "\n",
    "    # 6. Calculate the number of NAICS employees by TAZ.\n",
    "    taz_long['naics_emp_num'] = taz_long['sandag_emp_num'] * taz_long['Emp_PCT']\n",
    "\n",
    "    # 7. Aggregate NAICS employee number by TAZ\n",
    "    taz_long = taz_long.groupby(['taz', 'FAZ', 'NAICS_Emp'], as_index=False).agg({'naics_emp_num': 'sum'})\n",
    "\n",
    "    # 8. Calculate how many percentage of each emp category is within a TAZ\n",
    "    taz_long['emp_naics_perc'] = taz_long['naics_emp_num'] / taz_long.groupby(['NAICS_Emp', 'FAZ'])['naics_emp_num'].transform('sum')\n",
    "    taz_long = taz_long.loc[taz_long['naics_emp_num'] > 0]\n",
    "\n",
    "    return taz_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faf_disaggregate_to_taz(df, faz_gateway, cg_emp_a, cg_emp_p, faz_county, taz, annual_factor):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - df (DataFrame): faf data with aggregated commodity\n",
    "    - faz_gateway (DataFrame): DataFrame mapping FAZ to SADNAG gateways.\n",
    "    - faz_county (DataFrame): DataFrame containing FAZ and County information.\n",
    "\n",
    "    Returns:\n",
    "    - processed_df (DataFrame): Processed DataFrame with daily tonnage by TAZ origin and destination.\n",
    "\n",
    "    Description:\n",
    "    This function performs the following steps:\n",
    "    1. Determine NAICS emp category for production and attraction in FAF.\n",
    "    2. Bring TAZ numbers and the percentage of each emp category within each TAZ to relatively distribute tonnage to TAZ.\n",
    "    3. For FAZ outside the SANDAG region, assume that the distribution of tonnage is 1 for both attraction/production.\n",
    "    4. Calculate annual tonnage for each OD pair.\n",
    "    5. Reformat the faz_gateway table to long format using emp_ columns and pop.\n",
    "    6. Assign Gateway TAZ and percentage to the DataFrame.\n",
    "    7. Calculate final tonnage where one end of trip is outside the SANDAG region (taz_a or taz_p is null) and assign corresponding gateways as taz_a or taz_p.\n",
    "    8. Group by TAZ attraction and production and sum up the annual tonnage.\n",
    "    9. Convert annual to daily tonnage . the annual to daily factor is basically the number of working days within a year.\n",
    "\n",
    "    The processed DataFrame is returned.\n",
    "    \"\"\"\n",
    "    faz_gateway = df_dict[faz_gateway]\n",
    "    cg_emp_a = df_dict[cg_emp_a]\n",
    "    cg_emp_p = df_dict[cg_emp_p]\n",
    "    faz_county = df_dict[faz_county]\n",
    "    annual_factor = df_dict[annual_factor]\n",
    "\n",
    "\n",
    "    # 1. Determine NAICS emp category for production and attraction in FAF\n",
    "    cg_to_emp_a = cg_emp_a.set_index('CG')['Emp_a'].to_dict()\n",
    "    cg_to_emp_p = cg_emp_p.set_index('CG')['Emp_p'].to_dict()\n",
    "\n",
    "    # Use the map function to directly assign Emp_a and Emp_p values\n",
    "    df['Emp_a'] = df['CG'].map(cg_to_emp_a)\n",
    "    df['Emp_p'] = df['CG'].map(cg_to_emp_p)\n",
    "    \n",
    "    # 2. Bring TAZ numbers and the percentage of each emp category within each TAZ to relatively distribute tonnage to TAZ.\n",
    "    df = df.merge(taz[['taz', 'FAZ', 'NAICS_Emp', 'emp_naics_perc']], how='left', left_on=['Emp_p', 'dms_orig'], right_on=['NAICS_Emp', 'FAZ']).drop(['FAZ', 'NAICS_Emp', 'Emp_p'], axis=1)\n",
    "    df.rename(columns={'taz': 'taz_p', 'emp_naics_perc': 'emp_naics_perc_p'}, inplace=True)\n",
    "\n",
    "    df = df.merge(taz[['taz', 'FAZ', 'NAICS_Emp', 'emp_naics_perc']], how='left', left_on=['Emp_a', 'dms_dest'], right_on=['NAICS_Emp', 'FAZ']).drop(['FAZ', 'NAICS_Emp', 'Emp_a'], axis=1)\n",
    "    df.rename(columns={'taz': 'taz_a', 'emp_naics_perc': 'emp_naics_perc_a'}, inplace=True)\n",
    "\n",
    "    # 3. For FAZ outside the SANDAG region, assume that the distribution percentage is 1 for both attraction/production\n",
    "    # create a list of FAZ in San Diego\n",
    "    faz_sandag = faz_county[faz_county[\"County\"] == \"San Diego\"][\"FAZ\"]\n",
    "    df.loc[(~df['dms_orig'].isin(faz_sandag)) & (df['emp_naics_perc_p'].isnull()), 'emp_naics_perc_p'] = 1\n",
    "    df.loc[(~df['dms_dest'].isin(faz_sandag)) & (df['emp_naics_perc_a'].isnull()), 'emp_naics_perc_a'] = 1\n",
    "    \n",
    "    # 4. Calculate annual tonnage for each OD pair\n",
    "    df['dist_perc'] = df['emp_naics_perc_a'] * df['emp_naics_perc_p']\n",
    "    df['ton_annual'] = df['ton'] * df['dist_perc'] * 1000  # FAF data is in thousand tons\n",
    "    df.drop(['ton', 'emp_naics_perc_p', 'emp_naics_perc_a', 'dist_perc'], axis=1, inplace=True)\n",
    "    \n",
    "    # 5. Reformat the faz_gateway table to long format using emp_ columns and pop\n",
    "    gateway_airport_port = faz_gateway.columns[4:]\n",
    "    faz_gateway_long = faz_gateway.melt(id_vars=['FAZ'], value_vars=gateway_airport_port, value_name='faz_gtw_perc', var_name='gateways').dropna(subset=['faz_gtw_perc'])\n",
    "\n",
    "    # 6. Assign Gateway TAZ and percentage to the DataFrame\n",
    "    df = df.merge(faz_gateway_long, how='left', left_on='dms_orig', right_on='FAZ').drop('FAZ', axis=1)\n",
    "    df.rename(columns={'gateways': 'gateways_p', 'faz_gtw_perc': 'faz_gtw_perc_p'}, inplace=True)\n",
    "\n",
    "    df = df.merge(faz_gateway_long, how='left', left_on='dms_dest', right_on='FAZ').drop('FAZ', axis=1)\n",
    "    df.rename(columns={'gateways': 'gateways_a', 'faz_gtw_perc': 'faz_gtw_perc_a'}, inplace=True)\n",
    "\n",
    "    # 7. Calculate final tonnage \n",
    "    # For the end that is within SANDAG region, gateway distributions should be ignored \n",
    "    df.loc[(df['dms_orig'].isin(faz_sandag)) & (df['faz_gtw_perc_p'].isnull()), 'faz_gtw_perc_p'] = 1\n",
    "    df.loc[(df['dms_dest'].isin(faz_sandag)) & (df['faz_gtw_perc_a'].isnull()), 'faz_gtw_perc_a'] = 1\n",
    "\n",
    "    df['faz_gtw_perc'] = df['faz_gtw_perc_p'] * df['faz_gtw_perc_a']\n",
    "    df['ton_tot'] = df['ton_annual'] * df['faz_gtw_perc']\n",
    "\n",
    "    # df.loc[df['taz_a'].isnull(), 'ton_tot'] = df.loc[df['taz_a'].isnull(), 'ton_annual'] * df.loc[df['taz_a'].isnull(), 'faz_gtw_perc_a']\n",
    "    df.loc[df['taz_a'].isnull(), 'taz_a'] = df.loc[df['taz_a'].isnull(), 'gateways_a']\n",
    "\n",
    "    # df.loc[df['taz_p'].isnull(), 'ton_tot'] = df.loc[df['taz_p'].isnull(), 'ton_annual'] * df.loc[df['taz_p'].isnull(), 'faz_gtw_perc_p']\n",
    "    df.loc[df['taz_p'].isnull(), 'taz_p'] = df.loc[df['taz_p'].isnull(), 'gateways_p']\n",
    "    \n",
    "    # 8. Group by TAZ attraction and production and sum up the annual tonnage\n",
    "    # Note that there is a column that shows if at least one end of the trip is within Orange County. This column will be used in future steps to determine the OD distance.\n",
    "    processed_df = df.groupby(['taz_a', 'taz_p', 'CG', 'one_end_orange', 'one_end_mx'], as_index=False).agg({'ton_tot': 'sum'})\n",
    "\n",
    "    # 9. Convert annual to daily tonnage . the annual to daily factor is basically the number of working days within a year.\n",
    "    annual_to_daily_factor = annual_factor['Factor'].values\n",
    "    processed_df['ton_daily'] = processed_df['ton_tot'] / annual_to_daily_factor\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    processed_df.drop(['ton_tot'], axis=1, inplace=True)\n",
    "\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_ton_to_truck_by_type_and_tod(df, traffic_skims, truck_dist, payload, time_of_day):\n",
    "    \"\"\"\n",
    "    Convert daily tonnage to number of trucks by type and time of day.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): DataFrame containing the daily tonnage by TAZ.\n",
    "    - traffic_skims (DataFrame): DataFrame containing OD pair distance data.\n",
    "    - truck_dist (DataFrame): DataFrame containing truck type distribution data.\n",
    "    - payload (DataFrame): DataFrame containing payload data for truck types.\n",
    "    - time_of_day (DataFrame): DataFrame containing time of day factors.\n",
    "\n",
    "    Returns:\n",
    "    - final_df (DataFrame): Processed DataFrame with number of trucks by type and time of day (TOD).\n",
    "\n",
    "    Description:\n",
    "    This function performs the following steps:\n",
    "    1. Identify distance between two OD pairs within San Diego using skim.\n",
    "    2. Categorize the distance into different categories.\n",
    "        1. less than 50 miles; \n",
    "        2. 51 to 100 miles ; \n",
    "        3. One end in OC ; \n",
    "        4. 201 miles or more; or one end outside of SANDAG and Orange county regions.\n",
    "    3. Distribute daily tonnage between truck types based on OD distance.\n",
    "    4. Convert tonnage by truck type to the number of trucks using truck type and commodity.\n",
    "    5. Distribute the number of trucks by time of day.\n",
    "    The final processed DataFrame is returned.\n",
    "    \"\"\"\n",
    "    truck_dist = df_dict[truck_dist]\n",
    "    payload = df_dict[payload]\n",
    "    time_of_day = df_dict[time_of_day]\n",
    "\n",
    "    # 1. Identify distance between two OD pairs within San Diego using skim\n",
    "    df = df.merge(traffic_skims, how='left', left_on=['taz_a', 'taz_p'], right_on=['destination', 'origin']).drop(['origin', 'destination'], axis=1)\n",
    "\n",
    "    # 2. Categorize the distance\n",
    "    def categorize_dist(value):\n",
    "        if value <= 50:\n",
    "            return 1\n",
    "        elif value <= 100:\n",
    "            return 2\n",
    "        elif value <= 150:\n",
    "            return 3\n",
    "        elif value > 200:\n",
    "            return 4\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    df['dist_cat'] = np.where(df['one_end_orange'] == 1, 3, 0)\n",
    "    df['dist_cat'] = np.where(df['one_end_mx'] == 1, 5, 0)\n",
    "    df['dist_cat'] = np.where(df['dist_cat'] == 0, df['PM_TRK_H_DIST'].apply(categorize_dist), df['dist_cat'])\n",
    "    df['dist_cat'] = np.where(df['dist_cat'] == 0, 4, df['dist_cat'])\n",
    "    df.drop(['one_end_orange', 'PM_TRK_H_DIST'], axis=1, inplace=True)\n",
    "\n",
    "    # 3. Distribute daily tonnage between truck types based on OD distance\n",
    "    df = df.merge(truck_dist[['Dist_GP', 'Truck_Type', 'Dist']], how='left', left_on='dist_cat', right_on='Dist_GP').drop(['Dist_GP'], axis=1)\n",
    "    df['ton_daily_bytruck'] = df['ton_daily'] * df['Dist']\n",
    "    df.drop(['ton_daily', 'dist_cat', 'Dist'], axis=1, inplace=True)\n",
    "    df = df.groupby(['taz_a', 'taz_p', 'CG', 'Truck_Type'], as_index=False)['ton_daily_bytruck'].sum()\n",
    "\n",
    "    # 4. Convert tonnage by truck type to the number of trucks using truck type and commodity\n",
    "    max_tonnage_dict = payload.set_index(['CG', 'Truck_Type'])['Pounds'].to_dict()\n",
    "    df['Tonnage'] = df.apply(lambda row: max_tonnage_dict.get((row['CG'], row['Truck_Type']), 0), axis=1)\n",
    "    df = df.loc[df['Tonnage'] > 0]\n",
    "    df['tot_truck'] = (df['ton_daily_bytruck'] / df['Tonnage']) * 2000\n",
    "    df.drop(['ton_daily_bytruck', 'Tonnage'], axis=1, inplace=True)\n",
    "\n",
    "    # 5. Distribute the number of trucks by time of day\n",
    "    peak_periods = ['AM', 'MD', 'PM', 'EA', 'EV']\n",
    "    for period in peak_periods:\n",
    "        factor_column = time_of_day.loc[time_of_day['Peak_Period'] == period, 'Factor'].values\n",
    "        df[f'{period.lower()}_truck'] = df['tot_truck'] * factor_column\n",
    "\n",
    "    final_df = df\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mhasani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:312: UserWarning: Conditional Formatting extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/33 - 3.03% done\n",
      "Processing chunk 2/33 - 6.06% done\n",
      "Processing chunk 3/33 - 9.09% done\n",
      "Processing chunk 4/33 - 12.12% done\n",
      "Processing chunk 5/33 - 15.15% done\n",
      "Processing chunk 6/33 - 18.18% done\n",
      "Processing chunk 7/33 - 21.21% done\n",
      "Processing chunk 8/33 - 24.24% done\n",
      "Processing chunk 9/33 - 27.27% done\n",
      "Processing chunk 10/33 - 30.30% done\n",
      "Processing chunk 11/33 - 33.33% done\n",
      "Processing chunk 12/33 - 36.36% done\n",
      "Processing chunk 13/33 - 39.39% done\n",
      "Processing chunk 14/33 - 42.42% done\n",
      "Processing chunk 15/33 - 45.45% done\n",
      "Processing chunk 16/33 - 48.48% done\n",
      "Processing chunk 17/33 - 51.52% done\n",
      "Processing chunk 18/33 - 54.55% done\n",
      "Processing chunk 19/33 - 57.58% done\n",
      "Processing chunk 20/33 - 60.61% done\n",
      "Processing chunk 21/33 - 63.64% done\n",
      "Processing chunk 22/33 - 66.67% done\n",
      "Processing chunk 23/33 - 69.70% done\n",
      "Processing chunk 24/33 - 72.73% done\n",
      "Processing chunk 25/33 - 75.76% done\n",
      "Processing chunk 26/33 - 78.79% done\n",
      "Processing chunk 27/33 - 81.82% done\n",
      "Processing chunk 28/33 - 84.85% done\n",
      "Processing chunk 29/33 - 87.88% done\n",
      "Processing chunk 30/33 - 90.91% done\n",
      "Processing chunk 31/33 - 93.94% done\n",
      "Processing chunk 32/33 - 96.97% done\n",
      "Processing chunk 33/33 - 100.00% done\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and preprocess data\n",
    "df, mgra, traffic_skims, df_dict = load_and_preprocess_data(\"C:\\GitLab\\HVM scripts\\inputs_sandag_HTM.xlsx\")\n",
    "\n",
    "# Step 2: Prepare TAZ data\n",
    "taz = prepare_taz_data(mgra, 'faz_county', 'emp_converter', 'taz_faz')\n",
    "\n",
    "\n",
    "# Process df in chunks\n",
    "chunk_size = 100000\n",
    "num_chunks = len(df) // chunk_size + 1\n",
    "final_results = []\n",
    "final_ton = []\n",
    "\n",
    "for chunk_num in range(num_chunks):\n",
    "    start_idx = chunk_num * chunk_size\n",
    "    end_idx = (chunk_num + 1) * chunk_size\n",
    "    df_chunk = df[start_idx:end_idx]\n",
    "\n",
    "    # Step 3: Clean FAF data\n",
    "    df_chunk = clean_faf(df_chunk, 'faz_gateway', 'faz_county', 'sd_flows', 'othermode_truck', 'commodity_group')\n",
    "\n",
    "    # Step 4: FAF disaggregation to TAZ\n",
    "    df_chunk = faf_disaggregate_to_taz(df_chunk, 'faz_gateway', 'cg_emp_a', 'cg_emp_p', 'faz_county', taz, 'annual_factor')\n",
    "\n",
    "    # Step 5: Daily tonnage to truck types and time of day\n",
    "    final_chunk = daily_ton_to_truck_by_type_and_tod(df_chunk, traffic_skims, 'truck_dist', 'payload', 'time_of_day')\n",
    "    \n",
    "    # Append the results of the chunk to the final_results list\n",
    "    final_results.append(final_chunk)\n",
    "    final_ton.append(df_chunk)\n",
    "\n",
    "    # Print progress\n",
    "    progress = (chunk_num + 1) / num_chunks * 100\n",
    "    print(f\"Processing chunk {chunk_num + 1}/{num_chunks} - {progress:.2f}% done\")\n",
    "\n",
    "\n",
    "# Combine the results of all chunks\n",
    "final_truck = pd.concat(final_results, ignore_index=True)\n",
    "final_ton = pd.concat(final_ton, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine medium1 and 2 truck type to a medium type\n",
    "htm = final_truck\n",
    "htm.loc[htm['Truck_Type'] == \"Medium1\", 'Truck_Type'] = 'Medium'\n",
    "htm.loc[htm['Truck_Type'] == \"Medium2\", 'Truck_Type'] = 'Medium'\n",
    "\n",
    "htm = htm.groupby(['Truck_Type', 'taz_a', 'taz_p'], as_index=False)[['tot_truck', 'am_truck','md_truck', 'pm_truck', 'ea_truck', 'ev_truck']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the trip types: ei,ie (drop ii since it is not an HTM trip)\n",
    "external = [1,2,3,4,5,6,7,8,9,10,11,12, 1154, 1294, 1338, 1457,1476,1485,1520, 2086,2193,2372,2384,2497,3693,4184]\n",
    "htm['ei'] = 0\n",
    "htm['ie'] = 0\n",
    "htm.loc[(~htm.taz_a.isin(external)) & (htm.taz_p.isin(external)), 'ei'] = 1\n",
    "htm.loc[(htm.taz_a.isin(external)) & (~htm.taz_p.isin(external)), 'ie'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Truck_Type</th>\n",
       "      <th>taz_a</th>\n",
       "      <th>taz_p</th>\n",
       "      <th>tot_truck</th>\n",
       "      <th>am_truck</th>\n",
       "      <th>md_truck</th>\n",
       "      <th>pm_truck</th>\n",
       "      <th>ea_truck</th>\n",
       "      <th>ev_truck</th>\n",
       "      <th>ei</th>\n",
       "      <th>ie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Heavy</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3.009412</td>\n",
       "      <td>0.391224</td>\n",
       "      <td>0.752353</td>\n",
       "      <td>0.511600</td>\n",
       "      <td>0.481506</td>\n",
       "      <td>0.872729</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heavy</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.264617</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>0.066154</td>\n",
       "      <td>0.044985</td>\n",
       "      <td>0.042339</td>\n",
       "      <td>0.076739</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heavy</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>46.982100</td>\n",
       "      <td>6.107673</td>\n",
       "      <td>11.745525</td>\n",
       "      <td>7.986957</td>\n",
       "      <td>7.517136</td>\n",
       "      <td>13.624809</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heavy</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1126.521299</td>\n",
       "      <td>146.447769</td>\n",
       "      <td>281.630325</td>\n",
       "      <td>191.508621</td>\n",
       "      <td>180.243408</td>\n",
       "      <td>326.691177</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Heavy</td>\n",
       "      <td>3</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Truck_Type taz_a taz_p    tot_truck    am_truck    md_truck    pm_truck   \n",
       "0      Heavy     3     6     3.009412    0.391224    0.752353    0.511600  \\\n",
       "1      Heavy     3     7     0.264617    0.034400    0.066154    0.044985   \n",
       "2      Heavy     3    10    46.982100    6.107673   11.745525    7.986957   \n",
       "3      Heavy     3    12  1126.521299  146.447769  281.630325  191.508621   \n",
       "4      Heavy     3  13.0     0.000388    0.000050    0.000097    0.000066   \n",
       "\n",
       "     ea_truck    ev_truck  ei  ie  \n",
       "0    0.481506    0.872729   0   0  \n",
       "1    0.042339    0.076739   0   0  \n",
       "2    7.517136   13.624809   0   0  \n",
       "3  180.243408  326.691177   0   0  \n",
       "4    0.000062    0.000113   0   1  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Truck_Type\n",
       "Heavy     7374810\n",
       "Light     7374810\n",
       "Medium    7374810\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htm.Truck_Type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taz_a</th>\n",
       "      <th>taz_p</th>\n",
       "      <th>CG</th>\n",
       "      <th>one_end_orange</th>\n",
       "      <th>one_end_mx</th>\n",
       "      <th>ton_daily</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>14.0</td>\n",
       "      <td>CG-11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>14.0</td>\n",
       "      <td>CG-4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>14.0</td>\n",
       "      <td>CG-7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>14.0</td>\n",
       "      <td>CG-8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>14.0</td>\n",
       "      <td>CG-9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.032976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  taz_a taz_p     CG  one_end_orange  one_end_mx  ton_daily\n",
       "0    10  14.0  CG-11               0           0   0.022804\n",
       "1    10  14.0   CG-4               0           0   0.004018\n",
       "2    10  14.0   CG-7               0           0   0.042902\n",
       "3    10  14.0   CG-8               0           0   0.004867\n",
       "4    10  14.0   CG-9               0           0   0.032976"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ton.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrices for 'am_truck' saved as 'am_truck_matrices.omx'\n",
      "Matrices for 'md_truck' saved as 'md_truck_matrices.omx'\n",
      "Matrices for 'pm_truck' saved as 'pm_truck_matrices.omx'\n",
      "Matrices for 'ea_truck' saved as 'ea_truck_matrices.omx'\n",
      "Matrices for 'ev_truck' saved as 'ev_truck_matrices.omx'\n"
     ]
    }
   ],
   "source": [
    "# Define truck types and columns to process\n",
    "truck_types = htm['Truck_Type'].unique()\n",
    "columns_to_process = ['am_truck', 'md_truck', 'pm_truck', 'ea_truck', 'ev_truck']\n",
    "conditions = ['ei', 'ie']\n",
    "\n",
    "for column in columns_to_process:\n",
    "    matrices = {}\n",
    "    for condition in conditions:\n",
    "        condition_matrices = {}\n",
    "        for i, truck_type in enumerate(truck_types):\n",
    "            filtered_data = htm[(htm[condition] == 1) & (htm['Truck_Type'] == truck_type)]\n",
    "            filtered_data_group = filtered_data.groupby(['Truck_Type', 'taz_a', 'taz_p'], as_index=False)[[column]].sum()\n",
    "            matrix = filtered_data_group[['taz_a', 'taz_p', column]].pivot(index='taz_a', columns='taz_p', values=column).fillna(0)\n",
    "            condition_matrices[f\"{truck_type}_{condition}\"] = matrix.values\n",
    "        matrices[f\"{column}_{condition}_matrices\"] = condition_matrices\n",
    "\n",
    "    # Create OMX file for each column\n",
    "    output_directory = r\"C:\\GitLab\\HVM scripts\"  # Path to the directory\n",
    "    file_name = f\"{column}_matrices.omx\"\n",
    "    output_path = rf\"{output_directory}\\{file_name}\"\n",
    "    io.savemat(output_path, matrices)\n",
    "    print(f\"Matrices for '{column}' saved as '{file_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "final_truck.to_csv(\"HTM_Output.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ton.to_csv(\"Tonnage_Output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# file_path = \"C:/GitLab/HVM scripts/am_truck_matrices.omx\"  # Replace with the path to your OMX file\n",
    "# data = io.loadmat(file_path)\n",
    "# data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
