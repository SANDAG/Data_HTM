{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heavy Truck Model\n",
    "## FAF Annual Tonnage to Truck Number by TAZ Disaggregation\n",
    "\n",
    "Author: Maddie Hasani, Fehr & Peers <br/>\n",
    "Reviewer: Fatemeh Ranaiefar, Fehr & Peers<br/>\n",
    "Last update: 9/10/2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REQUIRED LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openmatrix as omx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(path):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - Path: a full path to the input file. For example: 'C:\\GitLab\\HVM scripts\\input_file.xlsx'\n",
    "\n",
    "    input_file:\n",
    "    - HTM input excel file\n",
    "\n",
    "    Returns:\n",
    "    - df_dict (dict): A dictionary containing DataFrames for each sheet in the input Excel file.\n",
    "    - traffic_skims (DataFrame): Processed DataFrame containing traffic skims.\n",
    "\n",
    "    Description:\n",
    "    This function loads and preprocesses input files and returns DataFrames and processed traffic skims.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a dictionary to hold DataFrames\n",
    "    df_dict = {}\n",
    "\n",
    "    # 1. Load in some inputs\n",
    "    input_path = path\n",
    "    inputs_sandag_HTM = pd.ExcelFile(input_path)\n",
    "    sheet_names = [sheet_name for sheet_name in inputs_sandag_HTM.sheet_names if sheet_name.lower() != 'userguide']\n",
    "\n",
    "    # 1.1 Load all sheets into separate DataFrames with lowercase names\n",
    "    for sheet_name in sheet_names:\n",
    "        df_name = sheet_name.lower()  # Convert sheet name to lowercase\n",
    "        df_dict[df_name] = inputs_sandag_HTM.parse(sheet_name)  # Save DataFrame to the dictionary\n",
    "\n",
    "    # 2. Load in FAF data\n",
    "    faf = df_dict['faf']  # Use the dictionary to get the DataFrame\n",
    "    faf_name = faf.loc[0, 'Name']\n",
    "    faf_path = faf.loc[0, 'Path']\n",
    "    full_faf_path = f\"{faf_path}\\\\{faf_name}.csv\"\n",
    "    df = pd.read_csv(full_faf_path)\n",
    "\n",
    "    # 3. Load in MGRA data\n",
    "    mgra_loc = df_dict['mgra']  # Use the dictionary to get the DataFrame\n",
    "    mgra_name = mgra_loc.loc[0, 'Name']\n",
    "    mgra_path = mgra_loc.loc[0, 'Path']\n",
    "    full_mgra_path = f\"{mgra_path}\\\\{mgra_name}.csv\"\n",
    "    mgra = pd.read_csv(full_mgra_path)\n",
    "\n",
    "    # 4. Load in Skim file\n",
    "    skim = df_dict['skim']  # Use the dictionary to get the DataFrame\n",
    "    skim_name = skim.loc[0, 'Name']\n",
    "    skim_path = skim.loc[0, 'Path']\n",
    "    full_skim_path = f\"{skim_path}\\\\{skim_name}.omx\"\n",
    "\n",
    "    traffic_skims_PM = []\n",
    "    with omx.open_file(full_skim_path) as omx_file:\n",
    "        for name in [\"PM_TRK_H_DIST\"]:\n",
    "            matrix = omx_file[name]\n",
    "            df_skim = pd.DataFrame(matrix[:])\n",
    "            stacked_df = df_skim.stack().reset_index()  # Reset the index to make it separate columns\n",
    "            stacked_df.columns = [\"origin\", \"destination\", name]  # Rename the columns\n",
    "            traffic_skims_PM.append(stacked_df)\n",
    "    traffic_skims = pd.concat(traffic_skims_PM, axis=1)\n",
    "\n",
    "    # 5. Filter out OD pairs where at least one end is a gateway\n",
    "    traffic_skims = traffic_skims.loc[(traffic_skims['origin'] >= 13) & (traffic_skims['destination'] >= 13)]\n",
    "\n",
    "    return df, mgra, traffic_skims, df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_faf(df, faz_county, othermode_truck, commodity_group):\n",
    "    \"\"\"\n",
    "    Process a DataFrame containing transportation data.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame containing transportation data.\n",
    "    - faz_county (DataFrame): DataFrame containing FAZ and County information.\n",
    "    - othermode_truck (DataFrame): DataFrame with mode information and percentages.\n",
    "    - commodity_group (DataFrame): DataFrame mapping commodities to CG values.\n",
    "\n",
    "    Returns:\n",
    "    - processed_df (DataFrame): Processed DataFrame with updated columns.\n",
    "\n",
    "    Description:\n",
    "    This function takes FAF data and performs the following steps:\n",
    "    1. Filters records based on FAZ within San Diego.\n",
    "    2. Includes only specified modes and calculates tonnage.\n",
    "    3. Deletes unnecessary columns.\n",
    "    4. Assigns SANDAG commodity groups based on SCTG commodity group.\n",
    "    5. Aggregates tonnage data by OD FAZ and Commodity Group.\n",
    "    6. Adds columns identifying if OD pairs have at least one end within Orange County.\n",
    "    The processed DataFrame is returned.\n",
    "    \"\"\"\n",
    "    faz_county = df_dict[faz_county]\n",
    "    othermode_truck = df_dict[othermode_truck]\n",
    "    commodity_group = df_dict[commodity_group]\n",
    "\n",
    "    # 1. FAZ within SD\n",
    "    faz_san_diego = faz_county[faz_county[\"County\"] == \"San Diego\"][\"FAZ\"]\n",
    "    df = df[(df[\"dms_orig\"].isin(faz_san_diego)) | (df[\"dms_dest\"].isin(faz_san_diego))]\n",
    "\n",
    "    # 2. Include some modes\n",
    "    mode_to_include = othermode_truck.set_index('Mode_Num')['Percentage'].to_dict()\n",
    "    df = df[df['Mode'].isin(mode_to_include.keys())]\n",
    "    df['truck_perc'] = df['Mode'].map(mode_to_include)\n",
    "    df['ton'] = df['distons_2017'] * df['truck_perc']\n",
    "    df.drop(['truck_perc', 'distons_2017'], axis=1, inplace=True)\n",
    "\n",
    "    # 3. Delete unnecessary columns\n",
    "    delete_col = ['disvalue_2017', 'Mode', 'fr_orig', 'fr_dest', 'fr_inmode', 'fr_outmode', 'Direction', 'Trade']\n",
    "    df.drop(delete_col, axis=1, inplace=True)\n",
    "\n",
    "    # 4. Assign SANDAG commodity groups based on SCTG commodity group\n",
    "    commodity_to_cg = commodity_group.set_index('SCTG')['CG'].to_dict()\n",
    "    df['CG'] = df['Commodity'].map(commodity_to_cg)\n",
    "    df.drop('Commodity', axis=1, inplace=True)\n",
    "\n",
    "    # 5. Aggregate the Tonnage Data by Origin/Destination and Commodity Group\n",
    "    df = df.groupby(['dms_orig', 'dms_dest', 'CG'], as_index=False).agg({'ton': 'sum'})\n",
    "\n",
    "    # Create a mapping dictionary for FAZ to County\n",
    "    faz_county_map = faz_county.set_index('FAZ')['County'].to_dict()\n",
    "    df['county_orig'] = df['dms_orig'].map(faz_county_map)\n",
    "    df['county_dest'] = df['dms_dest'].map(faz_county_map)\n",
    "\n",
    "    # 6. Create a new column that identifies if at least one end of the OD is within Orange County\n",
    "    df['one_end_orange'] = ((df['county_orig'] == \"Orange\") | (df['county_dest'] == \"Orange\")).astype(int)\n",
    "\n",
    "    # 7. Delete county_orig and county_dest columns\n",
    "    df.drop(['county_orig', 'county_dest'], axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_taz_data(mgra, emp_converter, taz_faz):\n",
    "    \"\"\"\n",
    "    Prepare TAZ data by calculating NAICS employee category percentages within each TAZ.\n",
    "\n",
    "    Parameters:\n",
    "    - mgra (DataFrame): DataFrame containing MGRA data.\n",
    "    - emp_converter (DataFrame): DataFrame mapping SANDAG emp category to NAICS emp category.\n",
    "    - taz_faz (DataFrame): DataFrame mapping TAZ values to FAZ values.\n",
    "\n",
    "    Returns:\n",
    "    - taz_long (DataFrame): Processed DataFrame with calculated employee category percentages.\n",
    "\n",
    "    Description:\n",
    "    This function calculates the percentage of each employee category (from the NAICS Dataset)\n",
    "    within each TAZ. It performs the following steps:\n",
    "    1. Calculate the number of SADNAG employees by TAZ.\n",
    "    2. Reformat the TAZ table to long format using emp_ and pop columns.\n",
    "    3. Convert SANDAG emp category to NAICS emp category.\n",
    "    4. Calculate the number of NAICS employees by TAZ.\n",
    "    5. Aggregate total number of each NAICS employee category by TAZ.\n",
    "    6. Create a mapping of TAZ values to FAZ values.\n",
    "    7. Calculate the NAICS employee percentage by comparing the number of each employee category within a TAZ to the corresponding number for the entire FAZ.\n",
    "    8. Filter out rows where the above percentage is zero.\n",
    "    The processed DataFrame is returned.\n",
    "    \"\"\"\n",
    "\n",
    "    emp_converter = df_dict[emp_converter]\n",
    "    taz_faz = df_dict[taz_faz]\n",
    "    \n",
    "    # 1. Calculate the number of employee by TAZ\n",
    "    cols_to_sum = ['pop'] + [col for col in mgra.columns if col.startswith('emp_')]\n",
    "    taz = mgra.groupby(['taz'], as_index=False)[cols_to_sum].sum()\n",
    "\n",
    "    # 2. Reformat the taz table to long format using emp_ columns and pop\n",
    "    sandag_emp_cat = emp_converter['SANDAG_Emp'].values\n",
    "    taz_long = taz.melt(id_vars=['taz'], value_vars=sandag_emp_cat, value_name='sandag_emp_num', var_name='sandag_emp')\n",
    "\n",
    "    # 3. Convert SANDAG emp category to NAICS emp category\n",
    "    taz_long = taz_long.merge(emp_converter, how='left', left_on='sandag_emp', right_on='SANDAG_Emp')\n",
    "\n",
    "    # 4. Calculate the number of NAICS employees by TAZ.\n",
    "    taz_long['naics_emp_num'] = taz_long['sandag_emp_num'] * taz_long['Percent']\n",
    "\n",
    "    # 5. Aggregate NAICS employee number by TAZ\n",
    "    taz_long = taz_long.groupby(['taz', 'NAICS_Emp'], as_index=False).agg({'naics_emp_num': 'sum'})\n",
    "\n",
    "    # 6. Create a dictionary to map taz values to TAZ values\n",
    "    taz_to_faz = taz_faz.set_index('TAZ')['FAZ'].to_dict()\n",
    "\n",
    "    # 7. Use the map function to directly assign TAZ values\n",
    "    taz_long['FAZ'] = taz_long['taz'].map(taz_to_faz)\n",
    "\n",
    "    # 8. Calculate how many percentage of each emp category is within a TAZ\n",
    "    taz_long['emp_naics_perc'] = taz_long['naics_emp_num'] / taz_long.groupby(['NAICS_Emp', 'FAZ'])['naics_emp_num'].transform('sum')\n",
    "    taz_long = taz_long.loc[taz_long['naics_emp_num'] > 0]\n",
    "\n",
    "    return taz_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faf_disaggregate_to_taz(df, faz_gateway, cg_emp_a, cg_emp_p, faz_county, taz, annual_factor):\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): faf data with aggregated commodity\n",
    "    - faz_gateway (DataFrame): DataFrame mapping FAZ to SADNAG gateways.\n",
    "\n",
    "    Returns:\n",
    "    - processed_df (DataFrame): Processed DataFrame with daily tonnage by TAZ origin and destination.\n",
    "\n",
    "    Description:\n",
    "    This function performs the following steps:\n",
    "    1. Determine NAICS emp category for production and attraction in FAF.\n",
    "    2. Bring TAZ numbers and the percentage of each emp category within each TAZ to relatively distribute tonnage to TAZ.\n",
    "    3. For FAZ outside the SANDAG region, assume that the distribution of tonnage is 1 for both attraction/production.\n",
    "    4. Calculate annual tonnage for each OD pair.\n",
    "    5. Reformat the faz_gateway table to long format using emp_ columns and pop.\n",
    "    6. Assign Gateway TAZ and percentage to the DataFrame.\n",
    "    7. Calculate final tonnage where one end of trip is outside the SANDAG region (taz_a or taz_p is null) and assign corresponding gateways as taz_a or taz_p.\n",
    "    8. Group by TAZ attraction and production and sum up the annual tonnage.\n",
    "    9. Convert annual to daily tonnage . the annual to daily factor is basically the number of working days within a year.\n",
    "\n",
    "    The processed DataFrame is returned.\n",
    "    \"\"\"\n",
    "    faz_gateway = df_dict[faz_gateway]\n",
    "    cg_emp_a = df_dict[cg_emp_a]\n",
    "    cg_emp_p = df_dict[cg_emp_p]\n",
    "    faz_county = df_dict[faz_county]\n",
    "    annual_factor = df_dict[annual_factor]\n",
    "\n",
    "\n",
    "    # 1. Determine NAICS emp category for production and attraction in FAF\n",
    "    cg_to_emp_a = cg_emp_a.set_index('CG')['Emp_a'].to_dict()\n",
    "    cg_to_emp_p = cg_emp_p.set_index('CG')['Emp_p'].to_dict()\n",
    "\n",
    "    # Use the map function to directly assign Emp_a and Emp_p values\n",
    "    df['Emp_a'] = df['CG'].map(cg_to_emp_a)\n",
    "    df['Emp_p'] = df['CG'].map(cg_to_emp_p)\n",
    "    \n",
    "    # 2. Bring TAZ numbers and the percentage of each emp category within each TAZ to relatively distribute tonnage to TAZ.\n",
    "    df = df.merge(taz[['taz', 'FAZ', 'NAICS_Emp', 'emp_naics_perc']], how='left', left_on=['Emp_p', 'dms_orig'], right_on=['NAICS_Emp', 'FAZ']).drop(['FAZ', 'NAICS_Emp', 'Emp_p'], axis=1)\n",
    "    df.rename(columns={'taz': 'taz_p', 'emp_naics_perc': 'emp_naics_perc_p'}, inplace=True)\n",
    "\n",
    "    df = df.merge(taz[['taz', 'FAZ', 'NAICS_Emp', 'emp_naics_perc']], how='left', left_on=['Emp_a', 'dms_dest'], right_on=['NAICS_Emp', 'FAZ']).drop(['FAZ', 'NAICS_Emp', 'Emp_a'], axis=1)\n",
    "    df.rename(columns={'taz': 'taz_a', 'emp_naics_perc': 'emp_naics_perc_a'}, inplace=True)\n",
    "\n",
    "    # 3. For FAZ outside the SANDAG region, assume that the distribution percentage is 1 for both attraction/production\n",
    "    # create a list of FAZ in San Diego\n",
    "    faz_san_diego = faz_county[faz_county[\"County\"] == \"San Diego\"][\"FAZ\"]\n",
    "    df.loc[(~df['dms_orig'].isin(faz_san_diego)) & (df['emp_naics_perc_p'].isnull()), 'emp_naics_perc_p'] = 1\n",
    "    df.loc[(~df['dms_dest'].isin(faz_san_diego)) & (df['emp_naics_perc_a'].isnull()), 'emp_naics_perc_a'] = 1\n",
    "    \n",
    "    # 4. Calculate annual tonnage for each OD pair\n",
    "    df['dist_perc'] = df['emp_naics_perc_a'] * df['emp_naics_perc_p']\n",
    "    df['ton_annual'] = df['ton'] * df['dist_perc'] * 1000  # FAF data is in thousand tons\n",
    "    df.drop(['ton', 'emp_naics_perc_p', 'emp_naics_perc_a', 'dist_perc'], axis=1, inplace=True)\n",
    "    \n",
    "    # 5. Reformat the faz_gateway table to long format using emp_ columns and pop\n",
    "    gateways = np.arange(1, 13)\n",
    "    faz_gateway_long = faz_gateway.melt(id_vars=['FAZ'], value_vars=gateways, value_name='faz_gtw_perc', var_name='gateways').dropna(subset=['faz_gtw_perc'])\n",
    "\n",
    "    # 6. Assign Gateway TAZ and percentage to the DataFrame\n",
    "    df = df.merge(faz_gateway_long, how='left', left_on='dms_orig', right_on='FAZ').drop('FAZ', axis=1)\n",
    "    df.rename(columns={'gateways': 'gateways_p', 'faz_gtw_perc': 'faz_gtw_perc_p'}, inplace=True)\n",
    "\n",
    "    df = df.merge(faz_gateway_long, how='left', left_on='dms_dest', right_on='FAZ').drop('FAZ', axis=1)\n",
    "    df.rename(columns={'gateways': 'gateways_a', 'faz_gtw_perc': 'faz_gtw_perc_a'}, inplace=True)\n",
    "\n",
    "    # 7. Calculate final tonnage where one end of trip is outside the SANDAG region (taz_a or taz_p is null) and assign corresponding gateways as taz_a or taz_p.\n",
    "    df.loc[df['taz_a'].isnull(), 'ton_tot'] = df.loc[df['taz_a'].isnull(), 'ton_annual'] * df.loc[df['taz_a'].isnull(), 'faz_gtw_perc_a']\n",
    "    df.loc[df['taz_a'].isnull(), 'taz_a'] = df.loc[df['taz_a'].isnull(), 'gateways_a']\n",
    "\n",
    "    df.loc[df['taz_p'].isnull(), 'ton_tot'] = df.loc[df['taz_p'].isnull(), 'ton_annual'] * df.loc[df['taz_p'].isnull(), 'faz_gtw_perc_p']\n",
    "    df.loc[df['taz_p'].isnull(), 'taz_p'] = df.loc[df['taz_p'].isnull(), 'gateways_p']\n",
    "    \n",
    "    # 8. Group by TAZ attraction and production and sum up the annual tonnage\n",
    "    # Note that there is a column that shows if at least one end of the trip is within Orange County. This column will be used in future steps to determine the OD distance.\n",
    "    processed_df = df.groupby(['taz_a', 'taz_p', 'CG', 'one_end_orange'], as_index=False).agg({'ton_tot': 'sum'})\n",
    "\n",
    "    # 9. Convert annual to daily tonnage . the annual to daily factor is basically the number of working days within a year.\n",
    "    annual_to_daily_factor = annual_factor['Factor'].values\n",
    "    processed_df['ton_daily'] = processed_df['ton_tot'] / annual_to_daily_factor\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    processed_df.drop(['ton_tot'], axis=1, inplace=True)\n",
    "\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_ton_to_truck_by_type_and_tod(df, traffic_skims, truck_dist, payload, time_of_day):\n",
    "    \"\"\"\n",
    "    Convert daily tonnage to number of trucks by type and time of day.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): DataFrame containing the daily tonnage by TAZ.\n",
    "    - traffic_skims (DataFrame): DataFrame containing OD pair distance data.\n",
    "    - truck_dist (DataFrame): DataFrame containing truck type distribution data.\n",
    "    - payload (DataFrame): DataFrame containing payload data for truck types.\n",
    "    - time_of_day (DataFrame): DataFrame containing time of day factors.\n",
    "\n",
    "    Returns:\n",
    "    - final_df (DataFrame): Processed DataFrame with number of trucks by type and time of day (TOD).\n",
    "\n",
    "    Description:\n",
    "    This function performs the following steps:\n",
    "    1. Identify distance between two OD pairs within San Diego using skim.\n",
    "    2. Categorize the distance into different categories.\n",
    "        1. less than 50 miles; \n",
    "        2. 51 to 100 miles ; \n",
    "        3. One end in OC ; \n",
    "        4. 201 miles or more; or one end outside of SANDAG and Orange county regions.\n",
    "    3. Distribute daily tonnage between truck types based on OD distance.\n",
    "    4. Convert tonnage by truck type to the number of trucks using truck type and commodity.\n",
    "    5. Distribute the number of trucks by time of day.\n",
    "    The final processed DataFrame is returned.\n",
    "    \"\"\"\n",
    "    truck_dist = df_dict[truck_dist]\n",
    "    payload = df_dict[payload]\n",
    "    time_of_day = df_dict[time_of_day]\n",
    "\n",
    "    # 1. Identify distance between two OD pairs within San Diego using skim\n",
    "    df = df.merge(traffic_skims, how='left', left_on=['taz_a', 'taz_p'], right_on=['destination', 'origin']).drop(['origin', 'destination'], axis=1)\n",
    "\n",
    "    # 2. Categorize the distance\n",
    "    def categorize_dist(value):\n",
    "        if value <= 50:\n",
    "            return 1\n",
    "        elif value <= 100:\n",
    "            return 2\n",
    "        elif value <= 150:\n",
    "            return 3\n",
    "        elif value > 200:\n",
    "            return 4\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    df['dist_cat'] = np.where(df['one_end_orange'] == 1, 3, 0)\n",
    "    df['dist_cat'] = np.where(df['dist_cat'] == 0, df['PM_TRK_H_DIST'].apply(categorize_dist), df['dist_cat'])\n",
    "    df['dist_cat'] = np.where(df['dist_cat'] == 0, 4, df['dist_cat'])\n",
    "    df.drop(['one_end_orange', 'PM_TRK_H_DIST'], axis=1, inplace=True)\n",
    "\n",
    "    # 3. Distribute daily tonnage between truck types based on OD distance\n",
    "    df = df.merge(truck_dist[['Dist_GP', 'Truck_Type', 'Dist']], how='left', left_on='dist_cat', right_on='Dist_GP').drop(['Dist_GP'], axis=1)\n",
    "    df['ton_daily_bytruck'] = df['ton_daily'] * df['Dist']\n",
    "    df.drop(['ton_daily', 'dist_cat', 'Dist'], axis=1, inplace=True)\n",
    "    df = df.groupby(['taz_a', 'taz_p', 'CG', 'Truck_Type'], as_index=False)['ton_daily_bytruck'].sum()\n",
    "\n",
    "    # 4. Convert tonnage by truck type to the number of trucks using truck type and commodity\n",
    "    max_tonnage_dict = payload.set_index(['CG', 'Truck_Type'])['Pounds'].to_dict()\n",
    "    df['Tonnage'] = df.apply(lambda row: max_tonnage_dict.get((row['CG'], row['Truck_Type']), 0), axis=1)\n",
    "    df = df.loc[df['Tonnage'] > 0]\n",
    "    df['tot_truck'] = (df['ton_daily_bytruck'] / df['Tonnage']) * 2000\n",
    "    df.drop(['ton_daily_bytruck', 'Tonnage'], axis=1, inplace=True)\n",
    "\n",
    "    # 5. Distribute the number of trucks by time of day\n",
    "    peak_periods = ['AM', 'MD', 'PM', 'EA', 'EV']\n",
    "    for period in peak_periods:\n",
    "        factor_column = time_of_day.loc[time_of_day['Peak_Period'] == period, 'Factor'].values\n",
    "        df[f'{period.lower()}_truck'] = df['tot_truck'] * factor_column\n",
    "\n",
    "    final_df = df\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/33 - 3.03% done\n",
      "Processing chunk 2/33 - 6.06% done\n",
      "Processing chunk 3/33 - 9.09% done\n",
      "Processing chunk 4/33 - 12.12% done\n",
      "Processing chunk 5/33 - 15.15% done\n",
      "Processing chunk 6/33 - 18.18% done\n",
      "Processing chunk 7/33 - 21.21% done\n",
      "Processing chunk 8/33 - 24.24% done\n",
      "Processing chunk 9/33 - 27.27% done\n",
      "Processing chunk 10/33 - 30.30% done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4372/4129185963.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Step 5: Daily tonnage to truck types and time of day\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mfinal_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdaily_ton_to_truck_by_type_and_tod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraffic_skims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'truck_dist'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'payload'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'time_of_day'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# Append the results of the chunk to the final_results list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4372/769436386.py\u001b[0m in \u001b[0;36mdaily_ton_to_truck_by_type_and_tod\u001b[1;34m(df, traffic_skims, truck_dist, payload, time_of_day)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# 1. Identify distance between two OD pairs within San Diego using skim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraffic_skims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'taz_a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'taz_p'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'destination'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'origin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'origin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'destination'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m# 2. Categorize the distance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mhasani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m   9846\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9848\u001b[1;33m         return merge(\n\u001b[0m\u001b[0;32m   9849\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9850\u001b[0m             \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mhasani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     )\n\u001b[1;32m--> 158\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mhasani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    803\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indicator_pre_merge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 805\u001b[1;33m         \u001b[0mjoin_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_join_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    806\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m         result = self._reindex_and_concat(\n",
      "\u001b[1;32mc:\\Users\\mhasani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_get_join_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1051\u001b[0m             )\n\u001b[0;32m   1052\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1053\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0mleft_indexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_indexer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_join_indexers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mhasani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_get_join_indexers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1024\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_join_indexers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNDArray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNDArray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m         \u001b[1;34m\"\"\"return the join indexers\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m         return get_join_indexers(\n\u001b[0m\u001b[0;32m   1027\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft_join_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\mhasani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mget_join_indexers\u001b[1;34m(left_keys, right_keys, sort, how, **kwargs)\u001b[0m\n\u001b[0;32m   1655\u001b[0m     \u001b[1;31m# set(lkey) | set(rkey) == range(count)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1657\u001b[1;33m     \u001b[0mlkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_factorize_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1658\u001b[0m     \u001b[1;31m# preserve left frame order if how == 'left' and sort == False\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mhasani\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_factorize_keys\u001b[1;34m(lk, rk, sort, how)\u001b[0m\n\u001b[0;32m   2409\u001b[0m         \u001b[1;31m# ndarray[Any, dtype[object_]]]\"; expected \"ndarray[Any, dtype[object_]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2410\u001b[0m         \u001b[0mllab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfactorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2411\u001b[1;33m         \u001b[0mrlab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfactorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2412\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mllab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mllab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2413\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mrlab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrlab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 1: Load and preprocess data\n",
    "df, mgra, traffic_skims, df_dict = load_and_preprocess_data(\"C:\\GitLab\\HVM scripts\\inputs_sandag_HTM.xlsx\")\n",
    "\n",
    "# Step 2: Prepare TAZ data\n",
    "taz = prepare_taz_data(mgra, 'emp_converter', 'taz_faz')\n",
    "\n",
    "\n",
    "# Process df in chunks\n",
    "chunk_size = 100000\n",
    "num_chunks = len(df) // chunk_size + 1\n",
    "final_results = []\n",
    "\n",
    "for chunk_num in range(num_chunks):\n",
    "    start_idx = chunk_num * chunk_size\n",
    "    end_idx = (chunk_num + 1) * chunk_size\n",
    "    df_chunk = df[start_idx:end_idx]\n",
    "\n",
    "    # Step 3: Clean FAF data\n",
    "    df_chunk = clean_faf(df_chunk, 'faz_county', 'othermode_truck', 'commodity_group')\n",
    "\n",
    "    # Step 4: FAF disaggregation to TAZ\n",
    "    df_chunk = faf_disaggregate_to_taz(df_chunk, 'faz_gateway', 'cg_emp_a', 'cg_emp_p', 'faz_county', taz, 'annual_factor')\n",
    "\n",
    "    # Step 5: Daily tonnage to truck types and time of day\n",
    "    final_chunk = daily_ton_to_truck_by_type_and_tod(df_chunk, traffic_skims, 'truck_dist', 'payload', 'time_of_day')\n",
    "    \n",
    "    # Append the results of the chunk to the final_results list\n",
    "    final_results.append(final_chunk)\n",
    "\n",
    "    # Print progress\n",
    "    progress = (chunk_num + 1) / num_chunks * 100\n",
    "    print(f\"Processing chunk {chunk_num + 1}/{num_chunks} - {progress:.2f}% done\")\n",
    "\n",
    "\n",
    "# Combine the results of all chunks\n",
    "final = pd.concat(final_results, ignore_index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
